{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 75.0k/75.0k [00:01<00:00, 47.3kB/s]\n",
      "Downloading data: 100%|██████████| 75.9k/75.9k [00:01<00:00, 48.6kB/s]\n",
      "Downloading data: 100%|██████████| 373k/373k [00:02<00:00, 181kB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80d478d9d589495f907c4e4033fb89ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a92f19bac54b53b5785f1df6daacd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44315a924e1244ee885972b76bc1a20b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 56.0k/56.0k [00:01<00:00, 42.9kB/s]\n",
      "Downloading data: 100%|██████████| 57.6k/57.6k [00:01<00:00, 43.8kB/s]\n",
      "Downloading data: 100%|██████████| 554k/554k [00:01<00:00, 292kB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98063dc983c04feebcfa7c7ca4e9366d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57666e8db0b241d1a2ec479d14c1af1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "109d879eb5004bde860dabb39212d3c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 92.4k/92.4k [00:01<00:00, 57.8kB/s]\n",
      "Downloading data: 100%|██████████| 92.7k/92.7k [00:01<00:00, 56.6kB/s]\n",
      "Downloading data: 100%|██████████| 1.34M/1.34M [00:02<00:00, 626kB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1662a96b66a4ac6a7e78204d8c9d159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f6a9b9f7a5d4bd2a1d4300b267cbc1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f54a779a81f64cb792e004f98e772923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/15000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 86.1k/86.1k [00:01<00:00, 56.9kB/s]\n",
      "Downloading data: 100%|██████████| 86.8k/86.8k [00:01<00:00, 51.2kB/s]\n",
      "Downloading data: 100%|██████████| 87.9k/87.9k [00:01<00:00, 58.5kB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff664c0ed224c378984c70a89c3d572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89a1ec05d21f4b558f08b58a76498a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc833c7621c743649077531be7549cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load hindi dataset\n",
    "hindi_dataset = load_dataset(\"wikiann\", \"hi\")\n",
    "\n",
    "# Load marathi data\n",
    "marathi_dataset = load_dataset(\"wikiann\", \"mr\")\n",
    "\n",
    "# Load bengali data\n",
    "bengali_dataset = load_dataset(\"wikiann\", \"bn\")\n",
    "\n",
    "# Load tamil data\n",
    "tamil_dataset = load_dataset(\"wikiann\", \"ta\")\n",
    "\n",
    "# Load telugu data\n",
    "telugu_dataset = load_dataset(\"wikiann\", \"te\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hindi: \n",
      "DatasetDict({\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Print the datasets\n",
    "print(\"Hindi: \")\n",
    "print(hindi_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marathi: \n",
      "DatasetDict({\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(\"Marathi: \")\n",
    "print(marathi_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bengali: \n",
      "DatasetDict({\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(\"Bengali: \")\n",
    "print(bengali_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamil: \n",
      "DatasetDict({\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 15000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(\"Tamil: \")\n",
    "print(tamil_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Telugu: \n",
      "DatasetDict({\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(\"Telugu: \")\n",
    "print(telugu_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Hindi Preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the train data for Hindi\n",
    "hindi_train = hindi_dataset['train']\n",
    "\n",
    "# Get the test data for Hindi\n",
    "hindi_test = hindi_dataset['test']\n",
    "\n",
    "hindi_train_lst = []\n",
    "\n",
    "# For each example in the train data, get the tokens and NER tags\n",
    "for example in hindi_train:\n",
    "    tokens = example['tokens']\n",
    "    ner_tags = example['ner_tags']\n",
    "\n",
    "    # Check if the tokens and NER tags are of the same length\n",
    "    # If not then skip this example\n",
    "    if len(tokens) != len(ner_tags):\n",
    "        continue\n",
    "\n",
    "    # Check if they are not of length 0\n",
    "    if len(tokens) == 0:\n",
    "        continue\n",
    "\n",
    "    temp_dict = {}\n",
    "    temp_dict['tokens'] = tokens\n",
    "    temp_dict['ner_tags'] = ner_tags\n",
    "    hindi_train_lst.append(temp_dict)\n",
    "\n",
    "hindi_test_lst = []\n",
    "\n",
    "# For each example in the test data, get the tokens and NER tags\n",
    "for example in hindi_test:\n",
    "    tokens = example['tokens']\n",
    "    ner_tags = example['ner_tags']\n",
    "\n",
    "    # Check if the tokens and NER tags are of the same length\n",
    "    # If not then skip this example\n",
    "    if len(tokens) != len(ner_tags):\n",
    "        continue\n",
    "\n",
    "    # Check if they are not of length 0\n",
    "    if len(tokens) == 0:\n",
    "        continue\n",
    "\n",
    "    temp_dict = {}\n",
    "    temp_dict['tokens'] = tokens\n",
    "    temp_dict['ner_tags'] = ner_tags\n",
    "    hindi_test_lst.append(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 4000 examples from the train data\n",
    "import random\n",
    "random.shuffle(hindi_train_lst)\n",
    "hindi_train_lst = hindi_train_lst[:4000]\n",
    "\n",
    "# Randomly select 1000 examples from the test data\n",
    "random.shuffle(hindi_test_lst)\n",
    "hindi_test_lst = hindi_test_lst[:1000]\n",
    "\n",
    "# Dump the train and test data to a json file\n",
    "import json\n",
    "\n",
    "with open('hindi_train.json', 'w') as f:\n",
    "    json.dump(hindi_train_lst, f)\n",
    "\n",
    "with open('hindi_test.json', 'w') as f:\n",
    "    json.dump(hindi_test_lst, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Marathi Preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "# Get the train data for Marathi\n",
    "marathi_train = marathi_dataset['train']\n",
    "\n",
    "# Get the test data for Marathi\n",
    "marathi_test = marathi_dataset['test']\n",
    "\n",
    "marathi_train_lst = []\n",
    "\n",
    "# For each example in the train data, get the tokens and NER tags\n",
    "for example in marathi_train:\n",
    "    tokens = example['tokens']\n",
    "    ner_tags = example['ner_tags']\n",
    "\n",
    "    # Check if the tokens and NER tags are of the same length\n",
    "    # If not then skip this example\n",
    "    if len(tokens) != len(ner_tags):\n",
    "        continue\n",
    "\n",
    "    # Check if they are not of length 0\n",
    "    if len(tokens) == 0:\n",
    "        continue\n",
    "\n",
    "    temp_dict = {}\n",
    "    temp_dict['tokens'] = tokens\n",
    "    temp_dict['ner_tags'] = ner_tags\n",
    "    marathi_train_lst.append(temp_dict)\n",
    "\n",
    "marathi_test_lst = []\n",
    "\n",
    "# For each example in the test data, get the tokens and NER tags\n",
    "for example in marathi_test:\n",
    "    tokens = example['tokens']\n",
    "    ner_tags = example['ner_tags']\n",
    "\n",
    "    # Check if the tokens and NER tags are of the same length\n",
    "    # If not then skip this example\n",
    "    if len(tokens) != len(ner_tags):\n",
    "        continue\n",
    "\n",
    "    # Check if they are not of length 0\n",
    "    if len(tokens) == 0:\n",
    "        continue\n",
    "\n",
    "    temp_dict = {}\n",
    "    temp_dict['tokens'] = tokens\n",
    "    temp_dict['ner_tags'] = ner_tags\n",
    "    marathi_test_lst.append(temp_dict)\n",
    "\n",
    "print(len(marathi_train_lst))\n",
    "print(len(marathi_test_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 4000 examples from the train data\n",
    "random.shuffle(marathi_train_lst)\n",
    "marathi_train_lst = marathi_train_lst[:4000]\n",
    "\n",
    "# Randomly select 1000 examples from the test data\n",
    "random.shuffle(marathi_test_lst)\n",
    "marathi_test_lst = marathi_test_lst[:1000]\n",
    "\n",
    "# Dump the train and test data to a json file\n",
    "with open('marathi_train.json', 'w') as f:\n",
    "    json.dump(marathi_train_lst, f)\n",
    "\n",
    "with open('marathi_test.json', 'w') as f:\n",
    "    json.dump(marathi_test_lst, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Bengali Preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "# Get the train data for Bengali\n",
    "bengali_train = bengali_dataset['train']\n",
    "\n",
    "# Get the test data for Bengali\n",
    "bengali_test = bengali_dataset['test']\n",
    "\n",
    "bengali_train_lst = []\n",
    "\n",
    "# For each example in the train data, get the tokens and NER tags\n",
    "for example in bengali_train:\n",
    "    tokens = example['tokens']\n",
    "    ner_tags = example['ner_tags']\n",
    "\n",
    "    # Check if the tokens and NER tags are of the same length\n",
    "    # If not then skip this example\n",
    "    if len(tokens) != len(ner_tags):\n",
    "        continue\n",
    "\n",
    "    # Check if they are not of length 0\n",
    "    if len(tokens) == 0:\n",
    "        continue\n",
    "\n",
    "    temp_dict = {}\n",
    "    temp_dict['tokens'] = tokens\n",
    "    temp_dict['ner_tags'] = ner_tags\n",
    "    bengali_train_lst.append(temp_dict)\n",
    "\n",
    "bengali_test_lst = []\n",
    "\n",
    "# For each example in the test data, get the tokens and NER tags\n",
    "for example in bengali_test:\n",
    "    tokens = example['tokens']\n",
    "    ner_tags = example['ner_tags']\n",
    "\n",
    "    # Check if the tokens and NER tags are of the same length\n",
    "    # If not then skip this example\n",
    "    if len(tokens) != len(ner_tags):\n",
    "        continue\n",
    "\n",
    "    # Check if they are not of length 0\n",
    "    if len(tokens) == 0:\n",
    "        continue\n",
    "\n",
    "    temp_dict = {}\n",
    "    temp_dict['tokens'] = tokens\n",
    "    temp_dict['ner_tags'] = ner_tags\n",
    "    bengali_test_lst.append(temp_dict)\n",
    "\n",
    "print(len(bengali_train_lst))\n",
    "print(len(bengali_test_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 4000 examples from the train data\n",
    "random.shuffle(bengali_train_lst)\n",
    "bengali_train_lst = bengali_train_lst[:4000]\n",
    "\n",
    "# Randomly select 1000 examples from the test data\n",
    "random.shuffle(bengali_test_lst)\n",
    "bengali_test_lst = bengali_test_lst[:1000]\n",
    "\n",
    "# Dump the train and test data to a json file\n",
    "with open('bengali_train.json', 'w') as f:\n",
    "    json.dump(bengali_train_lst, f)\n",
    "\n",
    "with open('bengali_test.json', 'w') as f:\n",
    "    json.dump(bengali_test_lst, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Tamil Preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "# Get the train data for Tamil\n",
    "tamil_train = tamil_dataset['train']\n",
    "\n",
    "# Get the test data for Tamil\n",
    "tamil_test = tamil_dataset['test']\n",
    "\n",
    "tamil_train_lst = []\n",
    "\n",
    "# For each example in the train data, get the tokens and NER tags\n",
    "for example in tamil_train:\n",
    "    tokens = example['tokens']\n",
    "    ner_tags = example['ner_tags']\n",
    "\n",
    "    # Check if the tokens and NER tags are of the same length\n",
    "    # If not then skip this example\n",
    "    if len(tokens) != len(ner_tags):\n",
    "        continue\n",
    "\n",
    "    # Check if they are not of length 0\n",
    "    if len(tokens) == 0:\n",
    "        continue\n",
    "\n",
    "    temp_dict = {}\n",
    "    temp_dict['tokens'] = tokens\n",
    "    temp_dict['ner_tags'] = ner_tags\n",
    "    tamil_train_lst.append(temp_dict)\n",
    "\n",
    "tamil_test_lst = []\n",
    "\n",
    "# For each example in the test data, get the tokens and NER tags\n",
    "for example in tamil_test:\n",
    "    tokens = example['tokens']\n",
    "    ner_tags = example['ner_tags']\n",
    "\n",
    "    # Check if the tokens and NER tags are of the same length\n",
    "    # If not then skip this example\n",
    "    if len(tokens) != len(ner_tags):\n",
    "        continue\n",
    "\n",
    "    # Check if they are not of length 0\n",
    "    if len(tokens) == 0:\n",
    "        continue\n",
    "\n",
    "    temp_dict = {}\n",
    "    temp_dict['tokens'] = tokens\n",
    "    temp_dict['ner_tags'] = ner_tags\n",
    "    tamil_test_lst.append(temp_dict)\n",
    "\n",
    "print(len(tamil_train_lst))\n",
    "print(len(tamil_test_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 4000 examples from the train data\n",
    "random.shuffle(tamil_train_lst)\n",
    "tamil_train_lst = tamil_train_lst[:4000]\n",
    "\n",
    "# Randomly select 1000 examples from the test data\n",
    "random.shuffle(tamil_test_lst)\n",
    "tamil_test_lst = tamil_test_lst[:1000]\n",
    "\n",
    "# Dump the train and test data to a json file\n",
    "with open('tamil_train.json', 'w') as f:\n",
    "    json.dump(tamil_train_lst, f)\n",
    "\n",
    "with open('tamil_test.json', 'w') as f:\n",
    "    json.dump(tamil_test_lst, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Telugu Preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "# Get the train data for Telugu\n",
    "telugu_train = telugu_dataset['train']\n",
    "\n",
    "# Get the valid data for Telugu\n",
    "telugu_valid = telugu_dataset['validation']\n",
    "\n",
    "telugu_train_lst = []\n",
    "\n",
    "# For each example in the train data, get the tokens and NER tags\n",
    "for example in telugu_train:\n",
    "    tokens = example['tokens']\n",
    "    ner_tags = example['ner_tags']\n",
    "\n",
    "    # Check if the tokens and NER tags are of the same length\n",
    "    # If not then skip this example\n",
    "    if len(tokens) != len(ner_tags):\n",
    "        continue\n",
    "\n",
    "    # Check if they are not of length 0\n",
    "    if len(tokens) == 0:\n",
    "        continue\n",
    "\n",
    "    temp_dict = {}\n",
    "    temp_dict['tokens'] = tokens\n",
    "    temp_dict['ner_tags'] = ner_tags\n",
    "    telugu_train_lst.append(temp_dict)\n",
    "\n",
    "# Also append everything in validation to train\n",
    "for example in telugu_valid:\n",
    "    tokens = example['tokens']\n",
    "    ner_tags = example['ner_tags']\n",
    "\n",
    "    # Check if the tokens and NER tags are of the same length\n",
    "    # If not then skip this example\n",
    "    if len(tokens) != len(ner_tags):\n",
    "        continue\n",
    "\n",
    "    # Check if they are not of length 0\n",
    "    if len(tokens) == 0:\n",
    "        continue\n",
    "\n",
    "    temp_dict = {}\n",
    "    temp_dict['tokens'] = tokens\n",
    "    temp_dict['ner_tags'] = ner_tags\n",
    "    telugu_train_lst.append(temp_dict)\n",
    "\n",
    "# Get the test data for Telugu\n",
    "telugu_test = telugu_dataset['test']\n",
    "\n",
    "telugu_test_lst = []\n",
    "\n",
    "# For each example in the test data, get the tokens and NER tags\n",
    "for example in telugu_test:\n",
    "    tokens = example['tokens']\n",
    "    ner_tags = example['ner_tags']\n",
    "\n",
    "    # Check if the tokens and NER tags are of the same length\n",
    "    # If not then skip this example\n",
    "    if len(tokens) != len(ner_tags):\n",
    "        continue\n",
    "\n",
    "    # Check if they are not of length 0\n",
    "    if len(tokens) == 0:\n",
    "        continue\n",
    "\n",
    "    temp_dict = {}\n",
    "    temp_dict['tokens'] = tokens\n",
    "    temp_dict['ner_tags'] = ner_tags\n",
    "    telugu_test_lst.append(temp_dict)\n",
    "\n",
    "print(len(telugu_train_lst))\n",
    "print(len(telugu_test_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 2000 examples from the train data\n",
    "random.shuffle(telugu_train_lst)\n",
    "telugu_train_lst = telugu_train_lst[:2000]\n",
    "\n",
    "# Randomly select 1000 examples from the test data\n",
    "random.shuffle(telugu_test_lst)\n",
    "telugu_test_lst = telugu_test_lst[:1000]\n",
    "\n",
    "# Dump the train and test data to a json file\n",
    "with open('telugu_train.json', 'w') as f:\n",
    "    json.dump(telugu_train_lst, f)\n",
    "\n",
    "with open('telugu_test.json', 'w') as f:\n",
    "    json.dump(telugu_test_lst, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLA3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
